# coding=utf-8
# Copyright 2018 The TF-Agents Authors.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# Lint as: python3
"""Utils for distributed training using Actor/Learner API."""

import time
from typing import Callable, Text, Tuple

from absl import logging

import tensorflow.compat.v2 as tf

from tf_agents.agents import tf_agent
from tf_agents.typing import types
from tf_agents.utils import lazy_loader

# Lazy loading since not all users have the reverb package installed.
reverb = lazy_loader.LazyLoader('reverb', globals(), 'reverb')


def create_train_step() -> tf.Variable:
  return tf.Variable(
      0,
      trainable=False,
      dtype=tf.int64,
      aggregation=tf.VariableAggregation.ONLY_FIRST_REPLICA,
      shape=())


def create_staleness_metrics_after_train_step_fn(
    train_step: tf.Variable,
    train_steps_per_policy_update: int = 1
) -> Callable[
    [Tuple[types.NestedTensor,
           types.ReverbSampleInfo], tf_agent.LossInfo], None]:
  """Creates an `after_train_step_fn` that computes staleness summaries.

  Staleness, in this context, means that the observation was generated by a
  policy that is older than the recently outputed policy.
  Assume that observation train step is stored as Reverb priorities.

  Args:
    train_step: The current train step.
    train_steps_per_policy_update: Number of train iterations to perform between
      two policy updates.

  Returns:
    The created `after_train_step_fn`.
  """

  def after_train_step_fn(experience, loss_info):
    del loss_info  # Unused.
    _, sample_info = experience

    # Get the train step in which the experience was observed. This is stored as
    # Reverb priority.
    # TODO(b/168426331): Check sample info version.
    observation_generation_train_step = tf.cast(
        sample_info.priority, dtype=tf.int64)

    # Get the train step corresponding to the latest outputed policy.
    # Policy is written in every `train_steps_per_policy_update` step, so we
    # normalize the value of `train_step` accordingly.
    on_policy_train_step = tf.cast(
        train_step / train_steps_per_policy_update,
        dtype=tf.int64) * train_steps_per_policy_update

    # An observation is off-policy if its train step delta is greater than 0.
    observation_train_step_delta = (
        on_policy_train_step - observation_generation_train_step)
    max_train_step_delta = tf.reduce_max(observation_train_step_delta)
    max_policy_update_delta = tf.cast(
        max_train_step_delta / train_steps_per_policy_update, dtype=tf.int64)
    num_stale_observations = tf.reduce_sum(
        tf.cast(observation_train_step_delta > 0, tf.int64))

    # Break out from local name scopes (e.g. the ones intrdouced by while loop).
    with tf.name_scope(''):
      # Write the summaries for the first replica.
      tf.summary.scalar(
          name='staleness/max_train_step_delta_in_batch',
          data=max_train_step_delta,
          step=train_step)
      tf.summary.scalar(
          name='staleness/max_policy_update_delta_in_batch',
          data=max_policy_update_delta,
          step=train_step)
      tf.summary.scalar(
          name='staleness/num_stale_obserations_in_batch',
          data=num_stale_observations,
          step=train_step)

  return after_train_step_fn


# TODO(b/142821173): Test train_utils `wait_for_files` functions.
def wait_for_file(file_path: Text,
                  sleep_time_secs: int = 2,
                  num_retries: int = 86400,
                  sleep: Callable[[int], None] = time.sleep) -> Text:
  """Blocks until the file at `file_path` becomes available.

  The default setting allows a fairly loose, but not infinite wait time of 2
  days for this function to block.

  Args:
    file_path: The path to the file that we are waiting for.
    sleep_time_secs: Number of time in seconds slept between retries.
    num_retries: Number of times the existence of the file is checked.
    sleep: Callable sleep function.

  Returns:
    The original `file_path`.

  Raises:
    TimeoutError: If the file does not become available during the number of
      trials.
  """
  retry = 0
  while (num_retries is None or
         retry < num_retries) and (not tf.io.gfile.exists(file_path) or
                                   tf.io.gfile.stat(file_path).length <= 0):
    logging.info('Waiting for the file to become available:\n\t%s', file_path)
    sleep(sleep_time_secs)
    retry += 1

  if (not tf.io.gfile.exists(file_path) or
      tf.io.gfile.stat(file_path).length <= 0):
    raise TimeoutError(
        'Could not find file {} after {} retries waiting {} seconds between '
        'retries.'.format(file_path, num_retries, sleep_time_secs))

  logging.info('The file %s became available, file length: %s', file_path,
               tf.io.gfile.stat(file_path).length)
  return file_path
