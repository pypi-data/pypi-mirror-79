Metadata-Version: 2.1
Name: responsibly
Version: 0.1.2
Summary: Toolkit for Auditing and Mitigating Bias and Fairness of Machine Learning Systems ðŸ”ŽðŸ¤–ðŸ§°
Home-page: https://docs.responsibly.ai
Author: Shlomi Hod
Author-email: shlomi.hod@gmail.com
License: MIT
Platform: UNKNOWN
Classifier: Development Status :: 4 - Beta
Classifier: Intended Audience :: Science/Research
Classifier: Topic :: Scientific/Engineering :: Artificial Intelligence
Classifier: Topic :: Text Processing :: Linguistic
Classifier: License :: OSI Approved :: MIT License
Classifier: Natural Language :: English
Classifier: Operating System :: OS Independent
Classifier: Programming Language :: Python
Classifier: Programming Language :: Python :: 3
Classifier: Programming Language :: Python :: 3 :: Only
Classifier: Programming Language :: Python :: 3.6
Classifier: Programming Language :: Python :: 3.7
Requires-Python: >=3.6
Description-Content-Type: text/x-rst
Requires-Dist: numpy (>=1.15)
Requires-Dist: scipy (>=1.1)
Requires-Dist: pandas (>=0.23)
Requires-Dist: matplotlib (<3,>=2.2)
Requires-Dist: seaborn (>=0.9)
Requires-Dist: scikit-learn (>=0.19)
Requires-Dist: gensim (>=3.7)
Requires-Dist: tabulate (>=0.8)
Requires-Dist: six (>=1.10)
Requires-Dist: click (>=6.0)
Requires-Dist: tqdm (>=4.24)
Requires-Dist: mlxtend (<0.17,>=0.13)

Responsibly
===========

.. image:: https://img.shields.io/badge/docs-passing-brightgreen.svg
    :target: https://docs.responsibly.ai

.. image:: https://img.shields.io/gitter/room/nwjs/nw.js.svg
   :alt: Join the chat at https://gitter.im/ResponsiblyAI/responsibly
   :target: https://gitter.im/ResponsiblyAI/responsibly

.. image:: https://img.shields.io/travis/ResponsiblyAI/responsibly/master.svg
    :target: https://travis-ci.org/ResponsiblyAI/responsibly

.. image::  https://img.shields.io/coveralls/ResponsiblyAI/responsibly/master.svg
   :target: https://coveralls.io/r/ResponsiblyAI/responsibly

.. image::  https://img.shields.io/scrutinizer/g/ResponsiblyAI/responsibly.svg
  :target: https://scrutinizer-ci.com/g/ResponsiblyAI/responsibly/?branch=master

.. image::  https://img.shields.io/pypi/v/responsibly.svg
  :target: https://pypi.org/project/responsibly

.. image::  https://img.shields.io/github/license/ResponsiblyAI/responsibly.svg
    :target: https://docs.responsibly.ai/about/license.html

**Toolkit for Auditing and Mitigating Bias and Fairness**
**of Machine Learning Systems ðŸ”ŽðŸ¤–ðŸ§°**

*Responsibly* is developed for **practitioners** and **researchers** in mind,
but also for learners. Therefore, it is compatible with
data science and machine learning tools of trade in Python,
such as Numpy, Pandas, and especially **scikit-learn**.

The primary goal is to be one-shop-stop for **auditing** bias
and fairness of machine learning systems, and the secondary one
is to mitigate bias and adjust fairness through
**algorithmic interventions**.
Besides, there is a particular focus on **NLP** models.

*Responsibly* consists of three sub-packages:

1. ``responsibly.dataset``
     Collection of common benchmark datasets from fairness research.

2. ``responsibly.fairness``
     Demographic fairness in binary classification,
     including metrics and algorithmic interventions.

3. ``responsibly.we``
     Metrics and debiasing methods for bias (such as gender and race)
     in word embedding.

For fairness, *Responsibly*'s functionality is aligned with the book
`Fairness and Machine Learning
- Limitations and Opportunities <https://fairmlbook.org>`_
by Solon Barocas, Moritz Hardt and Arvind Narayanan.

If you would like to ask for a feature or report a bug,
please open a
`new issue <https://github.com/ResponsiblyAI/responsibly/issues/new>`_
or write us in `Gitter <https://gitter.im/ResponsiblyAI/responsibly>`_.

Requirements
------------

-  Python 3.6+

Installation
------------

Install responsibly with pip:

.. code:: sh

   $ pip install responsibly

or directly from the source code:

.. code:: sh

   $ git clone https://github.com/ResponsiblyAI/responsibly.git
   $ cd responsibly
   $ python setup.py install

Citation
--------

If you have used *Responsibly* in a scientific publication,
we would appreciate citations to the following:

::

  @Misc{,
    author = {Shlomi Hod},
    title =  {{Responsibly}: Toolkit for Auditing and Mitigating Bias and Fairness of Machine Learning Systems},
    year =   {2018--},
    url =    "http://docs.responsibly.ai/",
    note =   {[Online; accessed <today>]}
  }

Revision History
================

0.1.2 (2020/09/15)
------------------

- Fix Travis CI issues with pipenv

- Fix bugs with word embedding bias

0.1.1 (2019/08/04)
------------------

- Fix a dependencies issue with ``smart_open``

- Change URLs to https

0.1.0 (2019/07/31)
------------------

- Rename the project to ``responsibly`` from ``ethically``

- Word embedding bias

  - Improve functionality of ``BiasWordEmbedding``

- Threshold fairness interventions

  - Fix bugs with ROCs handling
  - Improve API and add functionality (``plot_thresholds``)

0.0.5 (2019/06/14)
------------------

- Word embedding bias

  - Fix bug in computing WEAT

  - Computing and plotting factual property
    association to projections on a bias direction,
    similar to WEFAT


0.0.4 (2019/06/03)
------------------

- Word embedding bias

  - Unrestricted ``most_similar``

  - Unrestricted ``generate_analogies``

  - Running specific experiments with ``calc_all_weat``

  - Plotting clustering by classification
    of biased neutral words


0.0.3 (2019/04/10)
------------------

- Fairness in Classification

  - Three demographic fairness criteria

    - Independence
    - Separation
    - Sufficiency

  - Equalized odds post-processing algorithmic interventions
  - Complete two notebook demos (FICO and COMPAS)

- Word embedding bias

  - Measuring bias with WEAT method

- Documentation improvements

- Fixing security issues with dependencies


0.0.2 (2018/09/01)
------------------

- Word embedding bias

  - Generating analogies along the bias direction
  - Standard evaluations of word embedding (word pairs and analogies)
  - Plotting indirect bias
  - Scatter plot of bias direction projections between two word embedding
  - Improved verbose mode


0.0.1 (2018/08/17)
------------------

-  Gender debiasing for word embedding based on Bolukbasi et al.


